{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.lazy import CatLazyTensor, NonLazyTensor\n",
    "import torch, gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-batched cat dim 0 pass\n",
    "x = NonLazyTensor(torch.randn(5,1))\n",
    "y = NonLazyTensor(torch.randn(4,1))\n",
    "\n",
    "z = CatLazyTensor(*[x,y], dim=0)\n",
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-batched cat dim 1 pass\n",
    "x = NonLazyTensor(torch.randn(4,3))\n",
    "y = NonLazyTensor(torch.randn(4,2))\n",
    "\n",
    "#%debug\n",
    "z = CatLazyTensor(*[x,y], dim=1)\n",
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0777, -2.9289,  1.7987, -1.3053, -1.6413],\n",
       "        [-1.7167,  0.4824, -0.3797, -1.5872, -0.0787],\n",
       "        [ 0.8224,  1.3980,  0.2137, -1.7026, -0.8035],\n",
       "        [ 1.4306,  0.0311,  0.3998, -0.2568,  0.7621]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "All LazyTensors must have the same size in the non-concatenation dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3342269c7acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml/repos/gpytorch/gpytorch/lazy/cat_lazy_tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, *lazy_tensors)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tensors must have the same number of dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 raise RuntimeError(\"All LazyTensors must have the same size in \"\n\u001b[0m\u001b[1;32m     51\u001b[0m                                    \"the non-concatenation dimension\")\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Filter out empty LazyTensors, e.g. empty NonLazyTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: All LazyTensors must have the same size in the non-concatenation dimension"
     ]
    }
   ],
   "source": [
    "# Non-batched cat dim 0 fail\n",
    "x = NonLazyTensor(torch.randn(5,3))\n",
    "y = NonLazyTensor(torch.randn(4,2))\n",
    "\n",
    "z = CatLazyTensor(*[x,y], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "All LazyTensors must have the same size in the non-concatenation dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b20cba18dfda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml/repos/gpytorch/gpytorch/lazy/cat_lazy_tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, *lazy_tensors)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tensors must have the same number of dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 raise RuntimeError(\"All LazyTensors must have the same size in \"\n\u001b[0m\u001b[1;32m     51\u001b[0m                                    \"the non-concatenation dimension\")\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Filter out empty LazyTensors, e.g. empty NonLazyTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: All LazyTensors must have the same size in the non-concatenation dimension"
     ]
    }
   ],
   "source": [
    "# Non-batched cat dim 1 fail\n",
    "x = NonLazyTensor(torch.randn(5,3))\n",
    "y = NonLazyTensor(torch.randn(4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "x.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5,4,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[slice(0,1,None), slice(None, None, None), slice(None, None, None), slice(None, None, None)].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 1])\n",
      "> /home/alex/ml/repos/gpytorch/gpytorch/lazy/cat_lazy_tensor.py(94)_matmul()\n",
      "-> while rhs.ndimension() < self.ndimension():\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2167],\n",
       "         [ 0.1334],\n",
       "         [-0.3411],\n",
       "         [ 0.2939]],\n",
       "\n",
       "        [[-0.1123],\n",
       "         [ 0.0913],\n",
       "         [ 0.7820],\n",
       "         [ 0.8626]],\n",
       "\n",
       "        [[ 1.0736],\n",
       "         [ 1.1179],\n",
       "         [ 0.7341],\n",
       "         [ 0.9163]],\n",
       "\n",
       "        [[-0.0628],\n",
       "         [ 0.5939],\n",
       "         [ 0.3647],\n",
       "         [-0.2154]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched cat dim 0 pass\n",
    "x = NonLazyTensor(torch.randn(3,4,1))\n",
    "y = NonLazyTensor(torch.randn(1,4,1))\n",
    "\n",
    "#%debug\n",
    "z = CatLazyTensor(*[x,y], dim=0)\n",
    "print(z.size())\n",
    "z.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6934],\n",
       "         [-1.4482],\n",
       "         [-0.7071],\n",
       "         [-1.3278],\n",
       "         [-0.6392],\n",
       "         [-0.6384],\n",
       "         [ 0.8040],\n",
       "         [ 1.2791],\n",
       "         [-1.0186]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched cat dim 1 pass\n",
    "x = NonLazyTensor(torch.randn(1,5,1))\n",
    "y = NonLazyTensor(torch.randn(1,4,1))\n",
    "\n",
    "#%debug\n",
    "z = CatLazyTensor(*[x,y], dim=1)\n",
    "print(z.size())\n",
    "z.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6173, -1.6392,  0.3309,  0.6950,  1.8639],\n",
       "         [-0.3047,  0.0773, -0.6098, -0.2654,  0.0663],\n",
       "         [ 1.6305,  0.4748, -0.0281,  1.5121, -0.1897],\n",
       "         [-0.8223, -1.7555,  0.6705, -0.0978,  0.2077]],\n",
       "\n",
       "        [[-0.5612, -0.8767,  0.1047,  0.5907, -0.4869],\n",
       "         [-0.8295, -0.2056, -0.7326,  0.0462, -0.6485],\n",
       "         [ 1.4546,  1.6113,  0.8614, -0.8414,  0.7671],\n",
       "         [ 0.0742, -0.4846,  0.0099,  2.7439,  0.2789]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batched cat dim 2 pass\n",
    "x = NonLazyTensor(torch.randn(2,4,3))\n",
    "y = NonLazyTensor(torch.randn(2,4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=2)\n",
    "print(z.size())\n",
    "z.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "All LazyTensors must have the same size in the non-concatenation dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed39d32ef3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml/repos/gpytorch/gpytorch/lazy/cat_lazy_tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, *lazy_tensors)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tensors must have the same number of dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 raise RuntimeError(\"All LazyTensors must have the same size in \"\n\u001b[0m\u001b[1;32m     51\u001b[0m                                    \"the non-concatenation dimension\")\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Filter out empty LazyTensors, e.g. empty NonLazyTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: All LazyTensors must have the same size in the non-concatenation dimension"
     ]
    }
   ],
   "source": [
    "# Batched cat dim 1 fail\n",
    "x = NonLazyTensor(torch.randn(1, 5,3))\n",
    "y = NonLazyTensor(torch.randn(1, 4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "All LazyTensors must have the same size in the non-concatenation dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5814c78ddfa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNonLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml/repos/gpytorch/gpytorch/lazy/cat_lazy_tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, *lazy_tensors)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All tensors must have the same number of dimensions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mremove_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 raise RuntimeError(\"All LazyTensors must have the same size in \"\n\u001b[0m\u001b[1;32m     51\u001b[0m                                    \"the non-concatenation dimension\")\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Filter out empty LazyTensors, e.g. empty NonLazyTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: All LazyTensors must have the same size in the non-concatenation dimension"
     ]
    }
   ],
   "source": [
    "# Batched cat dim 2 fail\n",
    "x = NonLazyTensor(torch.randn(2, 5,3))\n",
    "y = NonLazyTensor(torch.randn(2, 4,2))\n",
    "\n",
    "z = CatLazyTensor(*[x,y], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5682, -0.4692,  1.9467,  0.6504,  1.5787, -1.2740, -0.7203, -0.5776,\n",
      "         0.8800])\n"
     ]
    }
   ],
   "source": [
    "# _matmul non-batched against 1D, dim=0\n",
    "x = NonLazyTensor(torch.randn(5,2))\n",
    "y = NonLazyTensor(torch.randn(4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=0)\n",
    "\n",
    "b = torch.randn(2)\n",
    "\n",
    "print(z._matmul(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0397,  0.9819,  2.6137],\n",
       "        [-0.6649, -0.2920,  1.2983],\n",
       "        [-0.6485,  0.9176,  1.7541],\n",
       "        [-0.2305, -0.6749,  0.2173],\n",
       "        [-1.8805,  2.9155,  5.1901],\n",
       "        [-0.8279, -0.8363,  1.4246],\n",
       "        [ 1.5897, -0.7003, -3.6715],\n",
       "        [ 0.8470, -2.0063, -2.6189],\n",
       "        [ 0.4620,  0.7941, -0.6621]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul non-batched against 2D , dim=0\n",
    "x = NonLazyTensor(torch.randn(5,2))\n",
    "y = NonLazyTensor(torch.randn(4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=0)\n",
    "\n",
    "b = torch.randn(2,3)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1308e-01,  1.0324e-02,  1.3557e-01, -8.6884e-02, -5.6084e-02,\n",
       "         -4.2278e-01, -6.8678e-02,  2.1316e-01, -1.0412e-01],\n",
       "        [-3.6976e-02, -2.6244e-02,  2.1939e-02, -3.0445e-04, -8.6439e-03,\n",
       "          7.3080e-02,  7.2223e-02, -5.6279e-02, -9.2961e-03],\n",
       "        [-6.6874e-03,  2.9258e-01, -1.8811e-01,  2.5883e-01, -1.4092e-04,\n",
       "         -2.6298e-02, -8.4113e-03, -7.9362e-03,  5.3528e-02]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul batched against 1D, dim=1\n",
    "x = NonLazyTensor(torch.randn(3,5,2))\n",
    "y = NonLazyTensor(torch.randn(3,4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=1)\n",
    "\n",
    "b = torch.randn(2)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6745e-01, -1.9775e-01, -7.4203e-01, -3.3731e+00, -6.8577e-01,\n",
       "          -5.9837e-01],\n",
       "         [-1.8873e-01,  2.4295e-01,  1.9252e-01,  2.9572e-01,  9.9126e-02,\n",
       "          -1.6060e-01],\n",
       "         [ 1.2610e+00, -1.8315e+00, -1.7465e+00, -3.8086e+00, -1.0523e+00,\n",
       "           8.4321e-01],\n",
       "         [ 6.5896e-01, -4.6355e-01,  1.7752e-01,  2.3516e+00,  3.7411e-01,\n",
       "           9.8511e-01],\n",
       "         [-1.7409e+00,  2.0945e+00,  1.4522e+00,  1.4388e+00,  6.4003e-01,\n",
       "          -1.6431e+00],\n",
       "         [-1.4900e+00,  1.4595e+00,  5.0731e-01, -1.6980e+00, -7.5672e-02,\n",
       "          -1.7736e+00],\n",
       "         [ 1.5061e+00, -1.6302e+00, -8.5493e-01,  3.5390e-01, -2.1348e-01,\n",
       "           1.6220e+00],\n",
       "         [ 2.0969e+00, -2.1030e+00, -8.2221e-01,  1.9585e+00,  1.4727e-02,\n",
       "           2.4419e+00],\n",
       "         [ 4.5337e-01, -5.9462e-01, -4.8679e-01, -8.0724e-01, -2.5873e-01,\n",
       "           3.7366e-01]],\n",
       "\n",
       "        [[-6.6169e-03,  2.8002e-01,  6.0642e-01,  2.3987e+00,  5.1176e-01,\n",
       "           2.9385e-01],\n",
       "         [ 9.8969e-01, -9.3829e-01, -2.6809e-01,  1.4022e+00,  1.0865e-01,\n",
       "           1.2125e+00],\n",
       "         [-2.7988e-02,  1.3336e-01,  2.4353e-01,  9.0003e-01,  1.9691e-01,\n",
       "           8.3546e-02],\n",
       "         [-1.0666e-01,  1.8477e-02, -1.5365e-01, -8.7812e-01, -1.6643e-01,\n",
       "          -2.2184e-01],\n",
       "         [ 4.9358e-01, -5.2259e-01, -2.5441e-01,  2.1856e-01, -4.8130e-02,\n",
       "           5.4440e-01],\n",
       "         [-1.8098e-01,  2.5465e-01,  2.3249e-01,  4.7427e-01,  1.3564e-01,\n",
       "          -1.3010e-01],\n",
       "         [ 7.7728e-02,  1.7551e-01,  5.2935e-01,  2.3022e+00,  4.7505e-01,\n",
       "           3.7010e-01],\n",
       "         [-1.3397e+00,  1.3499e+00,  5.3911e-01, -1.1964e+00,  2.2693e-03,\n",
       "          -1.5533e+00],\n",
       "         [ 3.3157e-01, -3.1971e-01, -1.0165e-01,  4.2267e-01,  2.6373e-02,\n",
       "           4.0031e-01]],\n",
       "\n",
       "        [[ 1.2822e+00, -1.4527e+00, -8.7101e-01, -2.6885e-01, -3.0309e-01,\n",
       "           1.3094e+00],\n",
       "         [-6.1894e-01,  7.3320e-01,  4.9104e-01,  4.1099e-01,  2.0615e-01,\n",
       "          -5.9677e-01],\n",
       "         [-3.0957e-01, -1.9046e-01, -9.8504e-01, -4.6956e+00, -9.3997e-01,\n",
       "          -9.1307e-01],\n",
       "         [-1.5064e+00,  1.4865e+00,  5.3685e-01, -1.6214e+00, -5.6216e-02,\n",
       "          -1.7812e+00],\n",
       "         [ 1.3257e+00, -1.2697e+00, -3.8746e-01,  1.7655e+00,  1.2152e-01,\n",
       "           1.6100e+00],\n",
       "         [-1.7012e-02,  3.6481e-01,  7.7473e-01,  3.0430e+00,  6.5089e-01,\n",
       "           3.6377e-01],\n",
       "         [ 1.4020e-01,  4.6084e-02,  3.5739e-01,  1.7733e+00,  3.5050e-01,\n",
       "           3.6922e-01],\n",
       "         [ 2.3640e-01, -4.5805e-01, -5.8073e-01, -1.7229e+00, -4.1199e-01,\n",
       "           3.1570e-02],\n",
       "         [ 1.7877e+00, -2.0782e+00, -1.3310e+00, -8.3948e-01, -5.2146e-01,\n",
       "           1.7673e+00]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul batched against 2D, dim=0\n",
    "x = NonLazyTensor(torch.randn(3,5,2))\n",
    "y = NonLazyTensor(torch.randn(3,4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=1)\n",
    "\n",
    "b = torch.randn(2,6)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1509,  0.8554,  0.6617])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul non-batched against 1D, dim=1\n",
    "x = NonLazyTensor(torch.randn(3,2))\n",
    "y = NonLazyTensor(torch.randn(3,5))\n",
    "z = CatLazyTensor(*[x,y], dim=1)\n",
    "\n",
    "b = torch.randn(7)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5197,  0.2915,  1.1359],\n",
       "        [ 1.8880, -0.1331, -1.0217],\n",
       "        [-1.0260, -0.1039,  1.2906],\n",
       "        [ 0.7460,  1.0556,  0.1110],\n",
       "        [-1.2223,  0.1739,  0.1970],\n",
       "        [ 2.4149, -0.7756,  0.7513],\n",
       "        [ 3.0334, -3.3928, -2.1592]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul non-batched against 2D , dim=1\n",
    "x = NonLazyTensor(torch.randn(7,3))\n",
    "y = NonLazyTensor(torch.randn(7,2))\n",
    "z = CatLazyTensor(*[x,y], dim=1)\n",
    "\n",
    "b = torch.randn(5,3)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0460,  1.1405,  3.7088, -3.5238],\n",
       "        [ 0.4248, -0.1310, -2.9800,  1.2447],\n",
       "        [ 4.2467, -2.2254,  3.6346,  1.2865]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul batched against 1D, dim=1\n",
    "x = NonLazyTensor(torch.randn(3,4,8))\n",
    "y = NonLazyTensor(torch.randn(3,4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=2)\n",
    "\n",
    "b = torch.randn(10)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1869],\n",
       "         [ 3.0840],\n",
       "         [-4.1718],\n",
       "         [ 5.6867]],\n",
       "\n",
       "        [[-0.6346],\n",
       "         [ 1.2843],\n",
       "         [ 7.0701],\n",
       "         [-1.1512]],\n",
       "\n",
       "        [[ 1.8299],\n",
       "         [ 1.1328],\n",
       "         [-3.2156],\n",
       "         [ 0.3970]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _matmul batched against 2D, dim=1\n",
    "x = NonLazyTensor(torch.randn(3,4,10))\n",
    "y = NonLazyTensor(torch.randn(3,4,2))\n",
    "z = CatLazyTensor(*[x,y], dim=2)\n",
    "\n",
    "b = torch.randn(12,1)\n",
    "\n",
    "z._matmul(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 12]) torch.Size([3, 12, 4])\n"
     ]
    }
   ],
   "source": [
    "# transpose\n",
    "x = NonLazyTensor(torch.randn(3,4,10))\n",
    "y = NonLazyTensor(torch.randn(3,4,2))\n",
    "\n",
    "print(z.size(), z.transpose(1, 2).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatLazyTensor of CatLazyTensors\n",
    "x = NonLazyTensor(torch.randn(5,1))\n",
    "y = NonLazyTensor(torch.randn(4,1))\n",
    "z = CatLazyTensor(*[x,y], dim=0)\n",
    "\n",
    "zz = CatLazyTensor(*[z,z], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0991, -0.0991],\n",
       "        [-0.3708, -0.3708],\n",
       "        [ 2.0858,  2.0858],\n",
       "        [ 1.5745,  1.5745],\n",
       "        [-0.3786, -0.3786],\n",
       "        [ 0.4450,  0.4450],\n",
       "        [ 0.1694,  0.1694],\n",
       "        [ 0.1170,  0.1170],\n",
       "        [-0.9882, -0.9882]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3493, -1.3493],\n",
      "        [ 0.5002,  0.5002],\n",
      "        [-0.6933, -0.6933],\n",
      "        [-0.1996, -0.1996],\n",
      "        [ 0.4352,  0.4352],\n",
      "        [-2.2446, -2.2446],\n",
      "        [-0.1572, -0.1572],\n",
      "        [ 0.9347,  0.9347],\n",
      "        [ 0.1795,  0.1795]])\n",
      "tensor([[-0.6933, -0.6933],\n",
      "        [-0.1996, -0.1996],\n",
      "        [ 0.4352,  0.4352]])\n"
     ]
    }
   ],
   "source": [
    "# Slice a CatLazyTensor\n",
    "x = NonLazyTensor(torch.randn(5,1))\n",
    "y = NonLazyTensor(torch.randn(4,1))\n",
    "z = CatLazyTensor(*[x,y], dim=0)\n",
    "\n",
    "zz = CatLazyTensor(*[z,z], dim=1)\n",
    "print(zz.evaluate())\n",
    "print(zz[2:5, :].evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
